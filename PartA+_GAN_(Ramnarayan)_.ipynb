{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON0oCYvC-Onv",
        "outputId": "c3b49860-27b9-4d57-cb18-909e934d316e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we will make a output directory in which the output will be stored"
      ],
      "metadata": {
        "id": "kdryJUhaPgWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path where you want to create the directory\n",
        "output_dir = '/content/drive/My Drive/my_output'"
      ],
      "metadata": {
        "id": "-p21NqGF-1_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the number of persons who have more than one image"
      ],
      "metadata": {
        "id": "cToho32uPw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import tarfile\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Define the path to your .tar dataset file\n",
        "tar_file = \"/content/drive/My Drive/lfw.tar\"\n",
        "\n",
        "# Create the output directory\n",
        "output_dir = \"/content/lfw\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Extract the .tar archive\n",
        "with tarfile.open(tar_file, 'r') as tar:\n",
        "    tar.extractall(output_dir)\n",
        "\n",
        "# Define the ratio for splitting (e.g., 60% train, 20% validation, 20% test)\n",
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "\n",
        "for root, dirs, files in os.walk(output_dir):\n",
        "    for file in files:\n",
        "        person_name = os.path.basename(root)\n",
        "        image_path = os.path.join(root, file)\n",
        "        person_images[person_name].append(image_path)\n",
        "\n",
        "# Get the number of persons with more than one image\n",
        "persons_with_multiple_images = [person for person, images in person_images.items() if len(images)>=2]\n",
        "num_persons_with_multiple_images = len(persons_with_multiple_images)\n",
        "print(f\"Number of persons with more than one image: {num_persons_with_multiple_images}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT3jBpcbm9ch",
        "outputId": "9de844e1-bedd-4772-f952-6527e6a0331d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of persons with more than one image: 5749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bT-Pc1DsllT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the directory you want to get the size of\n",
        "directory_path = \"/content/drive/MyDrive/my_output/train\"\n",
        "\n",
        "def get_directory_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            total_size += os.path.getsize(file_path)\n",
        "    return total_size\n",
        "\n",
        "directory_size_bytes = get_directory_size(directory_path)\n",
        "\n",
        "# Convert the size to a human-readable format (e.g., megabytes)\n",
        "def convert_bytes_to_megabytes(bytes):\n",
        "    megabytes = bytes / (1024 * 1024)\n",
        "    return megabytes\n",
        "\n",
        "directory_size_megabytes = convert_bytes_to_megabytes(directory_size_bytes)\n",
        "\n",
        "print(f\"Size of the directory '{directory_path}': {directory_size_bytes} bytes ({directory_size_megabytes} MB)\")\n"
      ],
      "metadata": {
        "id": "oX-mPH2oSLha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split images by person into train, validation, and test sets\n",
        "for person, images in person_images.items():\n",
        "    random.shuffle(images)\n",
        "    num_images = len(images)\n",
        "    num_train = int(train_ratio * num_images)\n",
        "    num_val = int(validation_ratio * num_images)\n",
        "\n",
        "    train_images = images[:num_train]\n",
        "    val_images = images[num_train:num_train + num_val]\n",
        "    test_images = images[num_train + num_val:]\n",
        "\n",
        "    # Define output directories for train, validation, and test sets\n",
        "    train_dir = os.path.join(output_dir, \"train\", person)\n",
        "    val_dir = os.path.join(output_dir, \"validation\", person)\n",
        "    test_dir = os.path.join(output_dir, \"test\", person)\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Copy images to the appropriate directories\n",
        "    for image in train_images:\n",
        "        dest_path = os.path.join(train_dir, os.path.basename(image))\n",
        "        if image != dest_path:\n",
        "            shutil.copy(image, dest_path)\n",
        "\n",
        "    for image in val_images:\n",
        "        dest_path = os.path.join(val_dir, os.path.basename(image))\n",
        "        if image != dest_path:\n",
        "            shutil.copy(image, dest_path)\n",
        "\n",
        "    for image in test_images:\n",
        "        dest_path = os.path.join(test_dir, os.path.basename(image))\n",
        "        if image != dest_path:\n",
        "            shutil.copy(image, dest_path)\n"
      ],
      "metadata": {
        "id": "EE-Y1pbRyUV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the directory you want to get the size of\n",
        "directory_path = \"/content/drive/MyDrive/my_output/test\"\n",
        "\n",
        "def get_directory_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            total_size += os.path.getsize(file_path)\n",
        "    return total_size\n",
        "\n",
        "directory_size_bytes = get_directory_size(directory_path)\n",
        "\n",
        "# Convert the size to a human-readable format (e.g., megabytes)\n",
        "def convert_bytes_to_megabytes(bytes):\n",
        "    megabytes = bytes / (1024 * 1024)\n",
        "    return megabytes\n",
        "\n",
        "directory_size_megabytes = convert_bytes_to_megabytes(directory_size_bytes)\n",
        "\n",
        "print(f\"Size of the directory '{directory_path}': {directory_size_bytes} bytes ({directory_size_megabytes} MB)\")\n"
      ],
      "metadata": {
        "id": "yyKJgUjRSq-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the directory you want to get the size of\n",
        "directory_path = \"/content/drive/MyDrive/my_output/validation\"\n",
        "\n",
        "def get_directory_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            total_size += os.path.getsize(file_path)\n",
        "    return total_size\n",
        "\n",
        "directory_size_bytes = get_directory_size(directory_path)\n",
        "\n",
        "# Convert the size to a human-readable format (e.g., megabytes)\n",
        "def convert_bytes_to_megabytes(bytes):\n",
        "    megabytes = bytes / (1024 * 1024)\n",
        "    return megabytes\n",
        "\n",
        "directory_size_megabytes = convert_bytes_to_megabytes(directory_size_bytes)\n",
        "\n",
        "print(f\"Size of the directory '{directory_path}': {directory_size_bytes} bytes ({directory_size_megabytes} MB)\")\n"
      ],
      "metadata": {
        "id": "E9_-gfa0Sxs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the directory you want to get the size of\n",
        "directory_path = \"/content/drive/MyDrive/my_output/lfw\"\n",
        "\n",
        "def get_directory_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            total_size += os.path.getsize(file_path)\n",
        "    return total_size\n",
        "\n",
        "directory_size_bytes = get_directory_size(directory_path)\n",
        "\n",
        "# Convert the size to a human-readable format (e.g., megabytes)\n",
        "def convert_bytes_to_megabytes(bytes):\n",
        "    megabytes = bytes / (1024 * 1024)\n",
        "    return megabytes\n",
        "\n",
        "directory_size_megabytes = convert_bytes_to_megabytes(directory_size_bytes)\n",
        "\n",
        "print(f\"Size of the directory '{directory_path}': {directory_size_bytes} bytes ({directory_size_megabytes} MB)\")\n"
      ],
      "metadata": {
        "id": "_RRAh64iS8Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load a pre-trained ResNet-50 model\n",
        "pretrained_model = models.resnet50(pretrained=True)\n"
      ],
      "metadata": {
        "id": "bEcDXjzIYxvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model = nn.Sequential(\n",
        "    pretrained_model.conv1,\n",
        "    pretrained_model.bn1,\n",
        "    pretrained_model.relu,\n",
        "    pretrained_model.maxpool,\n",
        "    pretrained_model.layer1,\n",
        "    pretrained_model.layer2,\n",
        "    pretrained_model.layer3,\n",
        "    pretrained_model.layer4,\n",
        "    nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
        ")\n",
        "\n",
        "# Define your own fully connected layer(s) for classification\n",
        "custom_model = nn.Sequential(\n",
        "    custom_model,\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(2048, 1000)  # Adjust num_classes to match your task\n",
        ")"
      ],
      "metadata": {
        "id": "EcbfajUGZ5cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the path to your image\n",
        "image_path = '/content/drive/MyDrive/my_output/lfw/AJ_Lamas/AJ_Lamas_0001.jpg'\n",
        "\n",
        "# Load the image using OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image is not None:\n",
        "    # Convert from BGR to RGB color space (since OpenCV loads images in BGR format)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.axis('off')  # Turn off the axis labels\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Failed to load the image.\")\n"
      ],
      "metadata": {
        "id": "1sMeMFYGpDMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model"
      ],
      "metadata": {
        "id": "twZa_KnSZ9jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Resize the image while preserving the aspect ratio\n",
        "    aspect_ratio = float(image.shape[1]) / image.shape[0]\n",
        "    if aspect_ratio > 1:\n",
        "        new_height = int(target_size / aspect_ratio)\n",
        "        image = cv2.resize(image, (target_size, new_height))\n",
        "    else:\n",
        "        new_width = int(target_size * aspect_ratio)\n",
        "        image = cv2.resize(image, (new_width, target_size))\n",
        "\n",
        "    # Center-crop the image to the target size\n",
        "    h, w = image.shape[:2]\n",
        "    top = (h - target_size) // 2\n",
        "    left = (w - target_size) // 2\n",
        "    image = image[top:top + target_size, left:left + target_size]\n",
        "\n",
        "    return image\n",
        "\n",
        "# Example usage:\n",
        "image_path = '/content/drive/MyDrive/my_output/lfw/AJ_Cook'\n",
        "target_size = 512\n",
        "preprocessed_image = preprocess_image(image_path, target_size)\n",
        "if preprocessed_image is not None:\n",
        "    # You can use the preprocessed_image for further processing or display\n",
        "    pass  # Replace with your code\n"
      ],
      "metadata": {
        "id": "UZ6TF1AFbyO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to your image\n",
        "image_path = '/content/drive/MyDrive/my_output/lfw/AJ_Lamas/AJ_Lamas_0001.jpg'\n",
        "\n",
        "# Load the image using OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image is not None:\n",
        "    print(\"Image loaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to load the image.\")\n"
      ],
      "metadata": {
        "id": "1SKH-zF0mQlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "FH6nVMRpnPSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the path to your image\n",
        "image_path = '/content/drive/MyDrive/my_output/lfw/AJ_Lamas/AJ_Lamas_0001.jpg'\n",
        "\n",
        "# Load the image using OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image is not None:\n",
        "    # Convert from BGR to RGB color space (since OpenCV loads images in BGR format)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.axis('off')  # Turn off the axis labels\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Failed to load the image.\")\n"
      ],
      "metadata": {
        "id": "3-Wmmuoko6hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a generative model for generating face images, using a GAN. The generator takes a Gaussian noise vector as input, and tries to output a face image, while the discriminator distinguishes between real and fake face images."
      ],
      "metadata": {
        "id": "COBGqhH_WCnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision"
      ],
      "metadata": {
        "id": "M1X6rj2zo6qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855f3d25-d632-470a-b7e2-c9a350e822e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "latent_dim = 100\n",
        "image_size = 64\n",
        "num_epochs = 10000\n",
        "lr = 0.0002\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "tar_file_path = '/content/drive/My Drive/lfw.tar'\n",
        "extract_path = '/content/lfw'  # Destination directory\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(tar_file_path, 'r') as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "# Define a custom dataset\n",
        "transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
        "                                transforms.ToTensor()])\n",
        "dataset = datasets.ImageFolder(root=extract_path, transform=transform)\n",
        "\n",
        "# Create data loader\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create the generator and discriminator and move them to the appropriate device\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Define the generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Define the discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Create the generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Define loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Lists to store generated images\n",
        "img_list = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop\n",
        "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)  # For generating fixed samples\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(data_loader, 0):\n",
        "        real_images, _ = data\n",
        "        batch_size = real_images.size(0)\n",
        "        real_label = torch.full((batch_size,), 1.0)\n",
        "        fake_label = torch.full((batch_size,), 0.0)\n",
        "\n",
        "        # Train the discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_images = real_images.to(device)\n",
        "        output = discriminator(real_images).view(-1)\n",
        "        errD_real = criterion(output, real_label)\n",
        "        errD_real.backward()\n",
        "\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_images = generator(noise)\n",
        "        output = discriminator(fake_images.detach()).view(-1)\n",
        "        errD_fake = criterion(output, fake_label)\n",
        "        errD_fake.backward()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train the generator\n",
        "        optimizer_G.zero_grad()\n",
        "        output = discriminator(fake_images).view(-1)\n",
        "        errG = criterion(output, real_label)\n",
        "        errG.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'[{epoch}/{num_epochs}][{i}/{len(data_loader)}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}')\n",
        "\n",
        "    if (epoch % 100 == 0) or ((epoch == num_epochs - 1)):\n",
        "        with torch.no_grad():\n",
        "            fake = generator(fixed_noise).detach().cpu()\n",
        "            img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(generator.state_dict(), '/content/drive/My Drive/generator.pth')\n",
        "torch.save(discriminator.state_dict(), '/content/drive/My Drive/discriminator.pth')\n",
        "\n",
        "# Save generated images\n",
        "for i in range(len(img_list)):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(np.transpose(img_list[i], (1, 2, 0)))\n",
        "    plt.savefig(f\"/content/drive/My Drive/gan_generated_image_epoch_{i}.png\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "5L2r8puYY5Z0",
        "outputId": "751b5930-b57c-4914-dfdb-351a9e25146a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-44123b090fa8>\u001b[0m in \u001b[0;36m<cell line: 106>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0merrD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0merrD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-44123b090fa8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Create the generator and discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "latent_dim = 100\n",
        "image_size = 64\n",
        "num_epochs = 10000\n",
        "lr = 0.0002\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "tar_file_path = '/content/drive/My Drive/lfw.tar'\n",
        "extract_path = '/content/lfw'  # Destination directory\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(tar_file_path, 'r') as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "# Define a custom dataset\n",
        "transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
        "                                transforms.ToTensor()])\n",
        "dataset = datasets.ImageFolder(root=extract_path, transform=transform)\n",
        "\n",
        "# Create data loader\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Define the discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Create the generator and discriminator and move them to the appropriate device\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Define loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Lists to store generated images\n",
        "img_list = []\n",
        "\n",
        "# Training loop# ...\n",
        "\n",
        "# Training loop\n",
        "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)  # For generating fixed samples\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(data_loader, 0):\n",
        "        real_images, _ = data\n",
        "        batch_size = real_images.size(0)\n",
        "        real_label = torch.full((batch_size, 1, 1, 1), 1.0, device=device)\n",
        "\n",
        "        # Ensure that fake_label matches the size of the output from the generator\n",
        "        fake_label = torch.full((batch_size, 1, 5, 5), 0.0, device=device)\n",
        "\n",
        "        # Train the discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_images = real_images.to(device)\n",
        "        output = discriminator(real_images)\n",
        "        errD_real = criterion(output, real_label)\n",
        "        errD_real.backward()\n",
        "\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_images = generator(noise)\n",
        "        output = discriminator(fake_images.detach())\n",
        "\n",
        "        # Ensure that fake_label matches the size of the output from the generator\n",
        "        fake_label = torch.full(output.shape, 0.0, device=device)\n",
        "\n",
        "        errD_fake = criterion(output, fake_label)\n",
        "        errD_fake.backward()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train the generator\n",
        "        optimizer_G.zero_grad()\n",
        "        output = discriminator(fake_images)\n",
        "\n",
        "        # Ensure that real_label matches the size of the output from the generator\n",
        "        real_label = torch.full(output.shape, 1.0, device=device)\n",
        "\n",
        "        errG = criterion(output, real_label)\n",
        "        errG.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'[{epoch}/{num_epochs}][{i}/{len(data_loader)}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    if (epoch % 100 == 0) or ((epoch == num_epochs - 1)):\n",
        "        with torch.no_grad():\n",
        "            fake = generator(fixed_noise).detach().cpu()\n",
        "            img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(generator.state_dict(), '/content/drive/My Drive/generator.pth')\n",
        "torch.save(discriminator.state_dict(), '/content/drive/My Drive/discriminator.pth')\n",
        "\n",
        "# Save generated images\n",
        "for i in range(len(img_list)):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(np.transpose(img_list[i], (1, 2, 0)))\n",
        "    plt.savefig(f\"/content/drive/My Drive/gan_generated_image_epoch_{i}.png\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "qKdVh0YicnLV"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}